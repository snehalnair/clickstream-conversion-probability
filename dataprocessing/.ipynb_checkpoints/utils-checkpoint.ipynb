{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set enviroment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"/Users/snehalnair/Workspace/blog/image_processing/imageenv/bin/python\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initiate spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('vla').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from pyspark.sql import SparkSession\n",
    "from typing import Iterable\n",
    "\n",
    "import pandas as pd\n",
    "from pyspark.ml import Pipeline, Transformer\n",
    "from pyspark.ml.feature import (\n",
    "    CountVectorizer,\n",
    "    StringIndexer,\n",
    "    VectorAssembler,\n",
    "    Binarizer,\n",
    "    HashingTF,\n",
    "    StandardScaler\n",
    ")\n",
    "from pyspark.ml.linalg import DenseVector, Vectors, VectorUDT\n",
    "from pyspark.sql import functions as fn\n",
    "from pyspark.sql.types import ArrayType, DoubleType, FloatType, IntegerType, StringType, BinaryType, StructType, StructField\n",
    "from pyspark.sql.window import Window\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elephas.ml_model import ElephasEstimator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stages = []\n",
    "class TrimByMinVisits(Transformer):\n",
    "        \"\"\"Remove visitors with less than declared minimum visits\n",
    "\n",
    "            Args:\n",
    "                None\n",
    "\n",
    "            Returns:\n",
    "                Update self.data_frame with trimmed dataset\n",
    "        \"\"\"\n",
    "        def __init__(self, minv: int):\n",
    "            self.minvisits = minv\n",
    "            super(TrimByMinVisits, self).__init__()\n",
    "        def _transform(self, df: fn.DataFrame) -> fn.DataFrame:\n",
    "            my_window = Window.partitionBy('fullVisitorId')\n",
    "            if self.minvisits > 1:\n",
    "                return df.select(\n",
    "                    \"*\", fn.count('fullVisitorId').\\\n",
    "                    over(my_window).alias(\"rank\")).where(\n",
    "                        \"rank > {}\".format(self.minvisits)\n",
    "                    )\n",
    "            return df \n",
    "stages += [TrimByMinVisits(MIN_VISITS)]\n",
    "\n",
    "class TimeBetweenVisits(Transformer):\n",
    "    \"\"\"Gets the time lapsed between two consecutive visits\n",
    "\n",
    "                Args:\n",
    "                    Transformer class\n",
    "\n",
    "                Returns:\n",
    "                    TimeBetweenVisits Transformer for the pipeline\n",
    "    \"\"\"\n",
    "    def _transform(self, df: fn.DataFrame) -> fn.DataFrame:\n",
    "            partition_col = 'fullVisitorId'\n",
    "            orderby_sort_col = 'visitStartTime'\n",
    "            my_window = Window.partitionBy(partition_col\n",
    "                ).orderBy(orderby_sort_col\n",
    "                )\n",
    "\n",
    "            return df.withColumn(\n",
    "                \"prev_value\", fn.lag(fn.col(orderby_sort_col)).over(my_window)\n",
    "                ).withColumn(\"timeBetweenVisits\",fn.when(fn.isnull(\n",
    "                      fn.col(orderby_sort_col) - fn.col('prev_value')), 0).otherwise(\n",
    "                      fn.col(orderby_sort_col) - fn.col('prev_value'))\n",
    "                ).drop('prev_value')\n",
    "stages += [TimeBetweenVisits()]     \n",
    "\n",
    "class AssertDataType(Transformer):\n",
    "    def _transform(self, df: fn.DataFrame) -> fn.DataFrame:\n",
    "        df.createOrReplaceTempView(\"df\")\n",
    "        return spark.sql(\"\"\"select \n",
    "          cast(fullVisitorId as string) as fullVisitorId, \n",
    "          cast(visitStartTime as int) as visitStartTime,\n",
    "          cast(channelGrouping as string) as channelGrouping,\n",
    "          cast(medium as string) as medium,\n",
    "          cast(source as string) as source,\n",
    "          cast(city as string) as city,\n",
    "          cast(deviceCategory as string) as deviceCategory,\n",
    "          cast(timeOnSite as float) as timeOnSite,\n",
    "          cast(timeBetweenVisits as float) as timeBetweenVisits,\n",
    "          cast(pagePathLevel1 as array<string>) as pagePathLevel1,\n",
    "          cast(pagePathLevel3 as array<string>) as pagePathLevel2,\n",
    "          cast(pagePathLevel3 as array<string>) as pagePathLevel3,\n",
    "          cast(contentGroup4 as array<string>) as contentGroup4,\n",
    "          cast(visitsIn0 as double) as visitsIn0,\n",
    "          cast(transactionsIn0 as double) as transactionsIn0\n",
    "          from df\"\"\")\n",
    "stages += [AssertDataType()]\n",
    "\n",
    "\n",
    "class FillNa(Transformer):\n",
    "    def _transform(self, df: fn.DataFrame) -> fn.DataFrame:\n",
    "      return df.fillna(0)\n",
    "stages += [FillNa()] \n",
    "\n",
    "\n",
    "scalervect_assembler = [VectorAssembler(\n",
    "    inputCols=[column], \n",
    "    outputCol=column + \"_vect\"\n",
    ") for column in VAR_TYPES['X_NUM_COLS']]\n",
    "stages += scalervect_assembler\n",
    "\n",
    "\n",
    "scaler = [StandardScaler(\n",
    "    inputCol=column + \"_vect\", \n",
    "    outputCol=column + \"_scaled\",\n",
    "    withStd=True, withMean=False,\n",
    ") for column in VAR_TYPES['X_NUM_COLS']]\n",
    "stages += scaler\n",
    "\n",
    "\n",
    "indexers = [StringIndexer(\n",
    "    inputCol=column, \n",
    "    outputCol=column + \"_index\"\n",
    ").setHandleInvalid(\"keep\") for column in VAR_TYPES['STRING_COLS']]\n",
    "stages += indexers\n",
    "\n",
    "\n",
    "hashingtf = [HashingTF(\n",
    "    inputCol=column, \n",
    "    outputCol=column + \"_vector\",\n",
    "    numFeatures=VOCABSIZE_ARRAYCOLS\n",
    ") for column in VAR_TYPES['ARRAY_COLS']]\n",
    "stages += hashingtf\n",
    "\n",
    "\n",
    "binarizer = [Binarizer(\n",
    "    inputCol=column, \n",
    "    outputCol=column + \"_binary\",\n",
    "    threshold=0\n",
    ") for column in VAR_TYPES['Y_COLS']]\n",
    "stages += binarizer\n",
    "\n",
    "class GroupBySortByStructPad(Transformer):\n",
    "    def __init__(self, finalcols_list: Iterable[str], padlen: int):\n",
    "        self.finalcols_list = finalcols_list\n",
    "        self.fcol_select = ['sortedByStartTime.'+colstr for colstr in self.finalcols_list]\n",
    "        self.padlength = padlen\n",
    "        super(GroupBySortByStructPad, self).__init__()\n",
    "    def _transform(self, df: fn.DataFrame) -> fn.DataFrame:\n",
    "        pad_plus_one = ['channelGrouping_index','medium_index','deviceCategory_index',\n",
    "                        'source_index', 'city_index','visitsIn0_binary',\n",
    "                        'transactionsIn0_binary']\n",
    "        for col in pad_plus_one:\n",
    "          df = df.withColumn(col, fn.col(col)+1)  \n",
    "        \n",
    "        win = Window.partitionBy('fullVisitorId').orderBy('visitStartTime')\n",
    "        return df.withColumn(\n",
    "          \"rank\", fn.dense_rank().over(win)\n",
    "        ).where(\n",
    "          'rank <= {}'.format(self.padlength)\n",
    "        ).groupBy('fullVisitorId').agg(\n",
    "          fn.array_sort(\n",
    "            fn.collect_list(\n",
    "              fn.struct(self.finalcols_list))).alias(\n",
    "          \"sortedByStartTime\"\n",
    "        )).select(self.fcol_select)\n",
    "        \n",
    "stages += [GroupBySortByStructPad(VAR_TYPES['FINAL_COLS'], PAD_LEN)]\n",
    "\n",
    "\n",
    "class PadVector(Transformer):\n",
    "    def zero_value(self, dataType):\n",
    "      if dataType == DoubleType():\n",
    "        return 0.0\n",
    "      elif dataType == VectorUDT():\n",
    "        return Vectors.sparse(VOCABSIZE_ARRAYCOLS, [])\n",
    "  \n",
    "    def __init__(self, pad_cols: Iterable[str], pad_len: int):\n",
    "      super(PadVector, self).__init__()\n",
    "      self.pad_cols = pad_cols\n",
    "      self.pad_len = pad_len\n",
    "        \n",
    "    def _transform(self, df: fn.DataFrame) -> fn.DataFrame:\n",
    "      field_datatype = {\n",
    "        f.name: f.dataType for f in df.schema.fields if f.name in self.pad_cols\n",
    "      }\n",
    "      schema_fields = df.schema.fields\n",
    "      i = df\n",
    "      for c in self.pad_cols:\n",
    "        padder = fn.udf(\n",
    "          lambda ar: ar + [ self.zero_value(field_datatype[c].elementType) \\\n",
    "                           for a in range(self.pad_len - len(ar))], field_datatype[c]\n",
    "        )\n",
    "        i = i.withColumn(c, padder(c))\n",
    "      return i\n",
    "        \n",
    "stages += [PadVector(pad_cols = VAR_TYPES['FINAL_COLS'], pad_len = 5)]\n",
    "\n",
    "class MergeArrayVector(Transformer):\n",
    "    \n",
    "    def _transform(self, df: fn.DataFrame) -> fn.DataFrame:\n",
    "        def merge_array_vector(arr):\n",
    "            if len(arr) == 0:\n",
    "                return arr\n",
    "            else:\n",
    "                f = arr[0].toArray()\n",
    "                for a in arr[1:]:\n",
    "                    f = f + a.toArray()\n",
    "                return Vectors.dense(f)\n",
    "        merge_array_vector_udf = fn.udf(merge_array_vector, VectorUDT())\n",
    "        return df.withColumn(\n",
    "            \"pagePath_vector_arr\",\n",
    "            fn.concat(\n",
    "                fn.col(\"pagePathLevel1_vector\"),\n",
    "                fn.col(\"pagePathLevel2_vector\"),\n",
    "                fn.col(\"pagePathLevel3_vector\"),\n",
    "            )\n",
    "        ).withColumn(\n",
    "            \"pagePath_vector\",\n",
    "            merge_array_vector_udf(fn.col(\"pagePath_vector_arr\"))\n",
    "        )\n",
    "stages += [MergeArrayVector()]\n",
    "\n",
    "class MergeIndexFeatures(Transformer):\n",
    "    def _transform(self, df: fn.DataFrame) -> fn.DataFrame:\n",
    "        array_to_vec = fn.udf(Vectors.dense, VectorUDT())\n",
    "        return df.withColumn(\n",
    "                \"index_features\",\n",
    "            array_to_vec(\n",
    "                fn.concat(\n",
    "                    fn.col(\"source_index\"),\n",
    "                    fn.col(\"medium_index\"),\n",
    "                    fn.col(\"channelGrouping_index\"),\n",
    "                    fn.col(\"deviceCategory_index\")\n",
    "        )\n",
    "            )\n",
    "                )\n",
    "stages += [MergeIndexFeatures()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit ('imageenv': venv)",
   "language": "python",
   "name": "python37364bitimageenvvenv1bd32f4f13d047a990e9b995ab17fcf9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
