{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run all the codes in the utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run parameters.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"/Users/snehalnair/Workspace/blog/image_processing/imageenv/bin/python\"\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = \"--packages com.linkedin.sparktfrecord:spark-tfrecord_2.11:0.2.1 pyspark-shell\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('vla').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run utils.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[__main__.BuildNN]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stageNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"quote\", \"\\\"\").option(\"escape\", \"\\\"\")\\\n",
    ".load(\"sampledata.csv\")\n",
    "\n",
    "def parse_array(s):\n",
    "    return ast.literal_eval(s)\n",
    "parse_array_udf = fn.udf(parse_array, ArrayType(StringType()))\n",
    "\n",
    "for col_ in ['pagePathLevel1', 'pagePathLevel2', 'pagePathLevel3', 'contentGroup4']:\n",
    "    df = df.withColumn(col_, parse_array_udf(fn.col(col_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_allfeatures = Pipeline(stages=stages[:-2]).fit(df).transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['timeOnSite_scaled',\n",
    " 'timeBetweenVisits_scaled',\n",
    " 'channelGrouping_index',\n",
    " 'deviceCategory_index',\n",
    " 'city_index',\n",
    " 'pagePathLevel1_vector',\n",
    " 'pagePathLevel2_vector',\n",
    " 'pagePathLevel3_vector',\n",
    " 'transactionsIn0_binary',\n",
    "]\n",
    "\n",
    "# df_allfeatures.select(cols).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lstm = Pipeline(stages=stages).fit(df).transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            features|label|\n",
      "+--------------------+-----+\n",
      "|(60,[0,5,10,15,24...|  1.0|\n",
      "|(60,[0,5,10,15,22...|  1.0|\n",
      "|(60,[0,5,10,15,20...|  2.0|\n",
      "|(60,[0,5,10,15,25...|  2.0|\n",
      "|(60,[0,5,10,15,26...|  1.0|\n",
      "|(60,[0,5,10,15,27...|  1.0|\n",
      "|(60,[0,5,10,15,24...|  1.0|\n",
      "|(60,[0,5,10,15,24...|  1.0|\n",
      "|(60,[0,5,10,15,32...|  1.0|\n",
      "|(60,[0,5,10,15,32...|  1.0|\n",
      "|(60,[0,5,10,15,24...|  1.0|\n",
      "|(60,[0,5,10,15,22...|  1.0|\n",
      "|(60,[0,5,10,15,24...|  2.0|\n",
      "|(60,[0,5,10,15,21...|  1.0|\n",
      "|(60,[0,5,10,15,21...|  1.0|\n",
      "|(60,[0,5,10,15,32...|  1.0|\n",
      "|(60,[0,5,10,15,24...|  1.0|\n",
      "|(60,[0,5,10,15,28...|  1.0|\n",
      "|(60,[0,5,10,15,24...|  1.0|\n",
      "|(60,[0,5,10,15,32...|  1.0|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_lstm.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o2012.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 20 in stage 54.0 failed 1 times, most recent failure: Lost task 20.0 in stage 54.0 (TID 480, sohans-mbp-3, executor driver): java.lang.IllegalArgumentException: Type org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7 does not support ordered operations\n\tat org.apache.spark.sql.catalyst.expressions.InterpretedOrdering.compare(ordering.scala:72)\n\tat org.apache.spark.sql.catalyst.expressions.InterpretedOrdering.compare(ordering.scala:38)\n\tat scala.math.Ordering.lt(Ordering.scala:97)\n\tat scala.math.Ordering.lt$(Ordering.scala:97)\n\tat org.apache.spark.sql.catalyst.expressions.BaseOrdering.lt(ordering.scala:29)\n\tat org.apache.spark.sql.catalyst.expressions.LessThan.nullSafeEval(predicates.scala:840)\n\tat org.apache.spark.sql.catalyst.expressions.BinaryExpression.eval(Expression.scala:552)\n\tat org.apache.spark.sql.catalyst.expressions.If.eval(conditionalExpressions.scala:60)\n\tat org.apache.spark.sql.catalyst.expressions.ArraySort.$anonfun$comparator$1(higherOrderFunctions.scala:362)\n\tat java.util.TimSort.countRunAndMakeAscending(TimSort.java:355)\n\tat java.util.TimSort.sort(TimSort.java:220)\n\tat java.util.Arrays.sort(Arrays.java:1438)\n\tat org.apache.spark.sql.catalyst.expressions.ArraySort.nullSafeEval(higherOrderFunctions.scala:369)\n\tat org.apache.spark.sql.catalyst.expressions.SimpleHigherOrderFunction.eval(higherOrderFunctions.scala:216)\n\tat org.apache.spark.sql.catalyst.expressions.SimpleHigherOrderFunction.eval$(higherOrderFunctions.scala:211)\n\tat org.apache.spark.sql.catalyst.expressions.ArraySort.eval(higherOrderFunctions.scala:314)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)\n\tat org.apache.spark.sql.execution.aggregate.AggregationIterator.$anonfun$generateResultProjection$5(AggregationIterator.scala:259)\n\tat org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.next(ObjectAggregationIterator.scala:86)\n\tat org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.next(ObjectAggregationIterator.scala:33)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:459)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:459)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:459)\n\tat scala.collection.Iterator$GroupedIterator.takeDestructively(Iterator.scala:1159)\n\tat scala.collection.Iterator$GroupedIterator.go(Iterator.scala:1174)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1215)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:295)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.writeIteratorToStream(PythonUDFRunner.scala:50)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:383)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1932)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:218)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2023)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1972)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1971)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1971)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:950)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:950)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:950)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2203)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2152)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2141)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:752)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2093)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2114)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2133)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1004)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1003)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:385)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:3448)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3616)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:763)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3614)\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3445)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.IllegalArgumentException: Type org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7 does not support ordered operations\n\tat org.apache.spark.sql.catalyst.expressions.InterpretedOrdering.compare(ordering.scala:72)\n\tat org.apache.spark.sql.catalyst.expressions.InterpretedOrdering.compare(ordering.scala:38)\n\tat scala.math.Ordering.lt(Ordering.scala:97)\n\tat scala.math.Ordering.lt$(Ordering.scala:97)\n\tat org.apache.spark.sql.catalyst.expressions.BaseOrdering.lt(ordering.scala:29)\n\tat org.apache.spark.sql.catalyst.expressions.LessThan.nullSafeEval(predicates.scala:840)\n\tat org.apache.spark.sql.catalyst.expressions.BinaryExpression.eval(Expression.scala:552)\n\tat org.apache.spark.sql.catalyst.expressions.If.eval(conditionalExpressions.scala:60)\n\tat org.apache.spark.sql.catalyst.expressions.ArraySort.$anonfun$comparator$1(higherOrderFunctions.scala:362)\n\tat java.util.TimSort.countRunAndMakeAscending(TimSort.java:355)\n\tat java.util.TimSort.sort(TimSort.java:220)\n\tat java.util.Arrays.sort(Arrays.java:1438)\n\tat org.apache.spark.sql.catalyst.expressions.ArraySort.nullSafeEval(higherOrderFunctions.scala:369)\n\tat org.apache.spark.sql.catalyst.expressions.SimpleHigherOrderFunction.eval(higherOrderFunctions.scala:216)\n\tat org.apache.spark.sql.catalyst.expressions.SimpleHigherOrderFunction.eval$(higherOrderFunctions.scala:211)\n\tat org.apache.spark.sql.catalyst.expressions.ArraySort.eval(higherOrderFunctions.scala:314)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)\n\tat org.apache.spark.sql.execution.aggregate.AggregationIterator.$anonfun$generateResultProjection$5(AggregationIterator.scala:259)\n\tat org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.next(ObjectAggregationIterator.scala:86)\n\tat org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.next(ObjectAggregationIterator.scala:33)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:459)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:459)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:459)\n\tat scala.collection.Iterator$GroupedIterator.takeDestructively(Iterator.scala:1159)\n\tat scala.collection.Iterator$GroupedIterator.go(Iterator.scala:1174)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1215)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:295)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.writeIteratorToStream(PythonUDFRunner.scala:50)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:383)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1932)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:218)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-58cd1d4342e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_lstm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoPandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/spark-3.0.0-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py\u001b[0m in \u001b[0;36mtoPandas\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0;31m# Below is toPandas without Arrow optimization.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m         \u001b[0mpdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_records\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m         \u001b[0mcolumn_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-3.0.0-bin-hadoop2.7/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    594\u001b[0m         \"\"\"\n\u001b[1;32m    595\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectToPython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    597\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBatchedSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPickleSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Workspace/blog/image_processing/imageenv/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-3.0.0-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Workspace/blog/image_processing/imageenv/lib/python3.7/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o2012.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 20 in stage 54.0 failed 1 times, most recent failure: Lost task 20.0 in stage 54.0 (TID 480, sohans-mbp-3, executor driver): java.lang.IllegalArgumentException: Type org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7 does not support ordered operations\n\tat org.apache.spark.sql.catalyst.expressions.InterpretedOrdering.compare(ordering.scala:72)\n\tat org.apache.spark.sql.catalyst.expressions.InterpretedOrdering.compare(ordering.scala:38)\n\tat scala.math.Ordering.lt(Ordering.scala:97)\n\tat scala.math.Ordering.lt$(Ordering.scala:97)\n\tat org.apache.spark.sql.catalyst.expressions.BaseOrdering.lt(ordering.scala:29)\n\tat org.apache.spark.sql.catalyst.expressions.LessThan.nullSafeEval(predicates.scala:840)\n\tat org.apache.spark.sql.catalyst.expressions.BinaryExpression.eval(Expression.scala:552)\n\tat org.apache.spark.sql.catalyst.expressions.If.eval(conditionalExpressions.scala:60)\n\tat org.apache.spark.sql.catalyst.expressions.ArraySort.$anonfun$comparator$1(higherOrderFunctions.scala:362)\n\tat java.util.TimSort.countRunAndMakeAscending(TimSort.java:355)\n\tat java.util.TimSort.sort(TimSort.java:220)\n\tat java.util.Arrays.sort(Arrays.java:1438)\n\tat org.apache.spark.sql.catalyst.expressions.ArraySort.nullSafeEval(higherOrderFunctions.scala:369)\n\tat org.apache.spark.sql.catalyst.expressions.SimpleHigherOrderFunction.eval(higherOrderFunctions.scala:216)\n\tat org.apache.spark.sql.catalyst.expressions.SimpleHigherOrderFunction.eval$(higherOrderFunctions.scala:211)\n\tat org.apache.spark.sql.catalyst.expressions.ArraySort.eval(higherOrderFunctions.scala:314)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)\n\tat org.apache.spark.sql.execution.aggregate.AggregationIterator.$anonfun$generateResultProjection$5(AggregationIterator.scala:259)\n\tat org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.next(ObjectAggregationIterator.scala:86)\n\tat org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.next(ObjectAggregationIterator.scala:33)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:459)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:459)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:459)\n\tat scala.collection.Iterator$GroupedIterator.takeDestructively(Iterator.scala:1159)\n\tat scala.collection.Iterator$GroupedIterator.go(Iterator.scala:1174)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1215)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:295)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.writeIteratorToStream(PythonUDFRunner.scala:50)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:383)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1932)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:218)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2023)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1972)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1971)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1971)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:950)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:950)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:950)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2203)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2152)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2141)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:752)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2093)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2114)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2133)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1004)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1003)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:385)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:3448)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3616)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:763)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3614)\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3445)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.IllegalArgumentException: Type org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7 does not support ordered operations\n\tat org.apache.spark.sql.catalyst.expressions.InterpretedOrdering.compare(ordering.scala:72)\n\tat org.apache.spark.sql.catalyst.expressions.InterpretedOrdering.compare(ordering.scala:38)\n\tat scala.math.Ordering.lt(Ordering.scala:97)\n\tat scala.math.Ordering.lt$(Ordering.scala:97)\n\tat org.apache.spark.sql.catalyst.expressions.BaseOrdering.lt(ordering.scala:29)\n\tat org.apache.spark.sql.catalyst.expressions.LessThan.nullSafeEval(predicates.scala:840)\n\tat org.apache.spark.sql.catalyst.expressions.BinaryExpression.eval(Expression.scala:552)\n\tat org.apache.spark.sql.catalyst.expressions.If.eval(conditionalExpressions.scala:60)\n\tat org.apache.spark.sql.catalyst.expressions.ArraySort.$anonfun$comparator$1(higherOrderFunctions.scala:362)\n\tat java.util.TimSort.countRunAndMakeAscending(TimSort.java:355)\n\tat java.util.TimSort.sort(TimSort.java:220)\n\tat java.util.Arrays.sort(Arrays.java:1438)\n\tat org.apache.spark.sql.catalyst.expressions.ArraySort.nullSafeEval(higherOrderFunctions.scala:369)\n\tat org.apache.spark.sql.catalyst.expressions.SimpleHigherOrderFunction.eval(higherOrderFunctions.scala:216)\n\tat org.apache.spark.sql.catalyst.expressions.SimpleHigherOrderFunction.eval$(higherOrderFunctions.scala:211)\n\tat org.apache.spark.sql.catalyst.expressions.ArraySort.eval(higherOrderFunctions.scala:314)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)\n\tat org.apache.spark.sql.execution.aggregate.AggregationIterator.$anonfun$generateResultProjection$5(AggregationIterator.scala:259)\n\tat org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.next(ObjectAggregationIterator.scala:86)\n\tat org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.next(ObjectAggregationIterator.scala:33)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:459)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:459)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:459)\n\tat scala.collection.Iterator$GroupedIterator.takeDestructively(Iterator.scala:1159)\n\tat scala.collection.Iterator$GroupedIterator.go(Iterator.scala:1174)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1215)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:295)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.writeIteratorToStream(PythonUDFRunner.scala:50)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:383)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1932)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:218)\n"
     ]
    }
   ],
   "source": [
    "df_lstm.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_lstm.write.format(\"tfrecords\").save(\"test.tfrecord\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Neural Network Layout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim_embed = df_allfeatures.limit(10).collect()[6][\"index_features\"].shape[0]\n",
    "output_dim_embed = 32\n",
    "input_length_embed = PAD_LEN\n",
    "n_features = df_allfeatures.limit(10).collect()[6][\"pagePath_vector\"].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build model layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lstm_model():\n",
    "    model = Sequential()\n",
    "\n",
    "    # Add Embedding layer\n",
    "    model.add(Embedding(input_dim=input_dim_embed, output_dim=output_dim_embed, input_length=input_length_embed))\n",
    "\n",
    "#     # Add Convolutional Layer\n",
    "    model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(PAD_LEN,n_features)))\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an estimator for spark using elphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 3, 32)             640       \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 1, 64)             6208      \n",
      "=================================================================\n",
      "Total params: 6,848\n",
      "Trainable params: 6,848\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "classes = 2\n",
    "epochs = 100\n",
    "batch_size = 200\n",
    "model = get_lstm_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_pipeline = Pipeline(stages=stageNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from systemml.mllearn import Keras2DML\n",
    "# # epochs = 5\n",
    "# # batch_size = 100\n",
    "# # samples = 1000\n",
    "# # max_iter = int(epochs*math.ceil(samples/batch_size))\n",
    "# # sysml_model = Keras2DML(spark, model, input_shape=(1,28,28), weights='weights_dir', batch_size=batch_size, max_iter=max_iter, test_interval=0, display=10)\n",
    "# # sysml_model.fit(X_train, y_train)\n",
    "# from sparkdl import KerasTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit ('imageenv': venv)",
   "language": "python",
   "name": "python37364bitimageenvvenv1bd32f4f13d047a990e9b995ab17fcf9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
